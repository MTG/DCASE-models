{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    " ____   ____    _    ____  _____                          _      _     \n",
    "|  _ \\ / ___|  / \\  / ___|| ____|     _ __ ___   ___   __| | ___| |___ \n",
    "| | | | |     / _ \\ \\___ \\|  _| _____| '_ ` _ \\ / _ \\ / _` |/ _ \\ / __|\n",
    "| |_| | |___ / ___ \\ ___) | |__|_____| | | | | | (_) | (_| |  __/ \\__ \\\n",
    "|____/ \\____/_/   \\_\\____/|_____|    |_| |_| |_|\\___/ \\__,_|\\___|_|___/\n",
    "                                                                        \n",
    "</pre>\n",
    "\n",
    "# DCASE-models Notebooks\n",
    "Python Notebooks for [DCASE-models](https://github.com/pzinemanas/DCASE-models)\n",
    "\n",
    "---\n",
    "\n",
    "### About \n",
    "This Notebook reproduces *some* results for **Enviromental Sound Classification** presented in:\n",
    "<ul>\n",
    "<li><a href=\"http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_cnn-aug-env_ieeespl_2017.pdf\"><strong>\n",
    "    Deep Convolutional Neural Networks and Data Augmentation For Environmental Sound Classification</strong></a>\n",
    "    J. Salamon and J. P. Bello IEEE Signal Processing Letters, 24(3), pages 279 - 283, 2017.\n",
    "    <br>\n",
    "   <a type=\"button\" class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_cnn-aug-env_ieeespl_2017.pdf\"> PDF </a>\n",
    "   <a type=\"button\" class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/7829341\"> IEEE</a>\n",
    "    </li>   \n",
    "</ul>\n",
    "\n",
    "### Overview\n",
    "\n",
    "The paper introduces an adaptation of the Convolutional Neural Network (CNN) for environmental\n",
    "sound classification, using [UrbanSound8k dataset](https://urbansounddataset.weebly.com/urbansound8k.html). To overcome data scarcity,the dataset is augmented through the following transformations:\n",
    "* Time stretching\n",
    "* Pitch shifting\n",
    "* Dynamic Range Compression\n",
    "* Background Noise\n",
    "\n",
    "This notebook aims to reproduce results regarding the augmented set *PS1*, obtained by pitch shifting each sample by 4 values (in semitones): {−2, −1, 1, 2}. Reported accuracy for all classes can be found on *figure 3* on the paper.\n",
    "\n",
    "### Organization\n",
    "\n",
    "The Notebook is organized into the following sections.\n",
    "* [1. Load parameters](#LoadParameters)\n",
    "* [2. Data augmentation](#DataAugmentation)\n",
    "* [3. Extract features](#ExtractFeatures)\n",
    "* [4. Load data](#LoadData)\n",
    "* [5. Initialize model](#InitModel)\n",
    "* [6. Train model](#TrainModel)\n",
    "* [7. Evaluate model](#EvaluateModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "rootdir_path = '../../'\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import glob\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "sys.path.append(rootdir_path)\n",
    "from dcase_models.utils.files import load_json, mkdir_if_not_exists\n",
    "from dcase_models.data.data_generator import DataGenerator\n",
    "from dcase_models.data.datasets import UrbanSound8k\n",
    "from dcase_models.data.scaler import Scaler\n",
    "from dcase_models.data.data_augmentation import AugmentedDataset\n",
    "from dcase_models.data.features import MelSpectrogram\n",
    "from dcase_models.model.models import SB_CNN\n",
    "from dcase_models.utils.data import evaluation_setup\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LoadeParameters\"></a>\n",
    "## 1. Load parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset, feature extraction, training and data augmentation default parameters are stored in a json file on the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all parameters from json file\n",
    "params = load_json(os.path.join(rootdir_path, 'parameters.json'))\n",
    "# set the dataset we are going to use\n",
    "dataset = 'UrbanSound8k'\n",
    "\n",
    "# get dataset parameters\n",
    "params_dataset = params[\"datasets\"][dataset]\n",
    "\n",
    "# get augmentation parameters\n",
    "params_augmentation = params[\"data_augmentations\"]\n",
    "\n",
    "# get feature extraction parameters\n",
    "params_features = params[\"features\"]\n",
    "\n",
    "# get training parameters\n",
    "params_train = params[\"train\"]\n",
    "\n",
    "params_model = params[\"models\"][\"SB_CNN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dataset parameters \n",
    "print(\"Dataset Parameters:\\n\", json.dumps(params_dataset, indent=4, sort_keys=True))\n",
    "# print augmentation parameters \n",
    "print(\"Augmentation Parameters:\\n\",json.dumps(params_augmentation, indent=4, sort_keys=True))\n",
    "# print feature extraction parameters \n",
    "print(\"Features' Parameters:\\n\",json.dumps(params_features, indent=4, sort_keys=True))\n",
    "# print training parameters \n",
    "print(\"Training Parameters:\\n\",json.dumps(params_train, indent=4, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DataAugmentation\"></a>\n",
    "## 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply one of the pitch shifting transformations described on the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data Generator as an instance of UrbanSound8k\n",
    "dataset = UrbanSound8k(os.path.join(rootdir_path, params_dataset[\"dataset_path\"]))\n",
    "# Download if needed\n",
    "dataset.download()\n",
    "# Set augmentation parameters to match values described on the paper\n",
    "semitones = [-2, -1, 1, 2]\n",
    "params_augmentation  = [{'type' : 'pitch_shift', 'n_semitones': s } for s in semitones]\n",
    "\n",
    "# Initialize AugmentedDataset\n",
    "aug_dataset = AugmentedDataset(dataset, 44100, params_augmentation)\n",
    "\n",
    "# Process all files\n",
    "print('Processing ...')\n",
    "aug_dataset.process()\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ExtractFeatures\"></a>\n",
    "## 2. Extract features\n",
    "\n",
    "Initialize Feature Extractor and extract features from the augmented dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define feature extraction params as described on the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params\n",
    "\n",
    "params_features = {\n",
    "    \"MelSpectrogram\": {\n",
    "        \"mel_bands\": 128,\n",
    "        \"n_fft\": 1024\n",
    "    },\n",
    "    \"audio_hop\": 1024,\n",
    "    \"audio_win\": 1024,\n",
    "    \"sequence_hop_time\": 1.0,\n",
    "    \"sequence_time\": 3.0,\n",
    "    \"sr\": 44100\n",
    "}\n",
    "# Initialize Feature Extractor\n",
    "features = MelSpectrogram(sequence_time=params_features['sequence_time'], \\\n",
    "                                   sequence_hop_time=params_features['sequence_hop_time'], \n",
    "                                   audio_win=params_features['audio_win'], \n",
    "                                   audio_hop=params_features['audio_hop'], \n",
    "                                   sr=params_features['sr'],\n",
    "                                   **params_features['MelSpectrogram'])\n",
    "print(features.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features (if they were not extracted before).\n",
    "if not features.check_if_extracted(aug_dataset):\n",
    "    features.extract(aug_dataset)\n",
    "print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LoadData\"></a>\n",
    "## 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test folds\n",
    "folds_train, folds_val, folds_test = evaluation_setup('test', dataset.fold_list,\\\n",
    "                                             params_dataset['evaluation_mode'],\n",
    "                                             use_validate_set=True)\n",
    "#initialise Data Generator\n",
    "data_gen_train = DataGenerator(dataset, features, folds=folds_train,\\\n",
    "                                batch_size=params_train['batch_size'],\n",
    "                                shuffle=True, train=True, scaler=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also fit a scaler to transform training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler(normalizer=params_model['normalizer'])\n",
    "print('Fitting features ...')\n",
    "scaler.fit(data_gen_train)\n",
    "print('Done!')\n",
    "\n",
    "data_gen_train.set_scaler(scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_val = DataGenerator(dataset, features, folds=folds_val,\\\n",
    "                             batch_size=params_train['batch_size'],\n",
    "                             shuffle=False, train=False, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"initmodel\"></a>\n",
    "## 6. Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_gen_train.get_data_batch(0)\n",
    " \n",
    "n_frames_cnn = X.shape[1]\n",
    "n_freq_cnn = X.shape[2]\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "\n",
    "print(n_frames_cnn, n_freq_cnn, n_classes)\n",
    "\n",
    "model_container = SB_CNN(model=None, model_path=None, n_classes=n_classes, \n",
    "                         n_frames_cnn=n_frames_cnn, n_freq_cnn=n_freq_cnn)\n",
    "\n",
    "model_container.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 7. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_folder = './'\n",
    "\n",
    "\n",
    "model_container.train(data_gen_train, data_gen_val, weights_path=exp_folder, **params_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Eval\"></a>\n",
    "## 8. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best_weights\n",
    "model_container.load_model_weights(exp_folder)\n",
    "\n",
    "data_gen_test = DataGenerator(dataset, features, folds=folds_test,\\\n",
    "                              batch_size=params_train['batch_size'],\\\n",
    "                              shuffle=False, train=False, scaler=scaler)\n",
    "\n",
    "kwargs = {'sequence_time_sec': params_features['sequence_hop_time'],\n",
    "          'metric_resolution_sec': 1.0}\n",
    "results = model_container.evaluate(data_gen_test, label_list=dataset.label_list, **kwargs)\n",
    "\n",
    "print(results[metrics[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
