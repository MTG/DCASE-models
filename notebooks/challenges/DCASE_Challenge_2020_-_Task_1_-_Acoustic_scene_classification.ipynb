{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    " ____   ____    _    ____  _____                          _      _     \n",
    "|  _ \\ / ___|  / \\  / ___|| ____|     _ __ ___   ___   __| | ___| |___ \n",
    "| | | | |     / _ \\ \\___ \\|  _| _____| '_ ` _ \\ / _ \\ / _` |/ _ \\ / __|\n",
    "| |_| | |___ / ___ \\ ___) | |__|_____| | | | | | (_) | (_| |  __/ \\__ \\\n",
    "|____/ \\____/_/   \\_\\____/|_____|    |_| |_| |_|\\___/ \\__,_|\\___|_|___/\n",
    "                                                                        \n",
    "</pre>\n",
    "\n",
    "# DCASE-models Notebooks\n",
    "Python Notebooks for [DCASE-models](https://github.com/pzinemanas/DCASE-models)\n",
    "\n",
    "---\n",
    "\n",
    "### About \n",
    "This Notebook reproduces the baseline sistem for Task 1 - subtask A from DCASE 2020 challenge presented in: \n",
    "<ul>\n",
    "<li><a href=\"https://arxiv.org/pdf/2005.14623.pdf\"><strong>\n",
    "    Acoustic scene classification in dcase 2020 challenge: generalization across devices and low complexity solutions.</strong></a>\n",
    "    Toni Heittola, Annamaria Mesaros, and Tuomas Virtanen. In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop. 2020.\n",
    "    <br>\n",
    "   <a type=\"button\" class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/2005.14623.pdf\"> Paper </a>\n",
    "   <a type=\"button\" class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dcase.community/challenge2020/task-acoustic-scene-classification\"> Challenge</a>\n",
    "    </li>   \n",
    "</ul>\n",
    "\n",
    "### Overview\n",
    "\n",
    "Subtask A in DCASE 2020 Acoustic Scene Clasification task focuses on generalization across different audio recording devices.  For this challenge a dataset with both real and simulated mobile recording devices, [TAU Urban Acoustic Scenes 2020 Mobile](https://zenodo.org/record/3670185), was released. The baseline system uses Open-L3 embeddings and two fully-connected feed-forward layers, architecture proposed by  Cramer et. al [[L3-Net]](http://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf).\n",
    "We aim to reproduce average (across devices) accuracy results.\n",
    "\n",
    "\n",
    "\n",
    "### Organization\n",
    "\n",
    "The Notebook is organized into the following sections.\n",
    "* [1. Load parameters](#LoadParameters)\n",
    "* [2. Extract features](#ExtractFeatures)\n",
    "* [3. Load data](#LoadData)\n",
    "* [4. Initialize model](#InitModel)\n",
    "* [5. Train model](#TrainModel)\n",
    "* [6. Evaluate model](#EvaluateModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from dcase_models.data.data_generator import DataGenerator\n",
    "from dcase_models.model.container import KerasModelContainer\n",
    "from dcase_models.data.datasets import TAUUrbanAcousticScenes2020Mobile\n",
    "from dcase_models.data.features import Openl3\n",
    "from dcase_models.data.scaler import Scaler\n",
    "from dcase_models.data.scaler import Scaler\n",
    "from dcase_models.util.files import load_json\n",
    "from dcase_models.util.data import evaluation_setup\n",
    "from dcase_models.util.files import load_json, mkdir_if_not_exists\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LoadeParameters\"></a>\n",
    "## 1. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features parameters\n",
    "sequence_time = 1.0\n",
    "sequence_hop_time = 0.1\n",
    "audio_hop = 512\n",
    "audio_win = 1024\n",
    "n_fft = 2048\n",
    "sr = 44100\n",
    "\n",
    "features_name = 'Openl3'\n",
    "features_params = {'content_type': 'music', \n",
    "                   'input_repr': 'mel256',\n",
    "                   'embedding_size': 512} \n",
    "\n",
    "# normalizer\n",
    "normalizer = 'minmax'\n",
    "\n",
    "# train parameters\n",
    "early_stopping = 20\n",
    "epochs = 200\n",
    "considered_improvement = 0\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "verbose = 1\n",
    "optimizer = 'Adam'\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = 'TAUUrbanAcousticScenes2020Mobile'\n",
    "dataset_path = \"datasets/TAUUrbanAcousticScenes2020Mobile\"\n",
    "evaluation_mode = \"train-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ExtractFeatures\"></a>\n",
    "## 2. Extract features\n",
    "\n",
    "Initialize Dataset and Feature Extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 512)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Feature Extractor\n",
    "features = Openl3(sequence_time=sequence_time, sequence_hop_time=sequence_hop_time,\n",
    "                  audio_win=audio_win, audio_hop=audio_hop, sr=44100,\n",
    "                  content_type=features_params[\"content_type\"],\n",
    "                  input_repr=features_params[\"input_repr\"],\n",
    "                  embedding_size=features_params[\"embedding_size\"])\n",
    "\n",
    "print(features.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "download() takes from 1 to 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5891b12f1d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTAUUrbanAcousticScenes2020Mobile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Download dataset if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DCASE-models/dcase_models/data/datasets.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, force_download)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         downloaded = super().download(\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mzenodo_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzenodo_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         )\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownloaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: download() takes from 1 to 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Initialize Data Generator as an instance of TAUUrbanAcousticScenes2020Mobile\n",
    "rootdir_path = '../../'\n",
    "dataset = TAUUrbanAcousticScenes2020Mobile(os.path.join(rootdir_path, dataset_path))\n",
    "# Download dataset if needed\n",
    "dataset.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing sampling rate ...\n",
      "Done!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../datasets/TAUUrbanAcousticScenes2020Mobile/features/Openl3/original/parameters.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-af531d08dd58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_if_extracted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DCASE-models/dcase_models/data/feature_extractor.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# Save parameters.json for future checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_as_extracted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_path_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_if_extracted_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DCASE-models/dcase_models/data/feature_extractor.py\u001b[0m in \u001b[0;36mset_as_extracted\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n\u001b[1;32m    173\u001b[0m         \u001b[0mjson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parameters.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../datasets/TAUUrbanAcousticScenes2020Mobile/features/Openl3/original/parameters.json'"
     ]
    }
   ],
   "source": [
    "if not features.check_if_extracted(dataset):\n",
    "    features.extract(dataset)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LoadData\"></a>\n",
    "## 3. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise train data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test folds\n",
    "folds_train, folds_val, folds_test = evaluation_setup('test', dataset.fold_list,\n",
    "                                                      evaluation_mode,\n",
    "                                                      use_validate_set=True)\n",
    "#initialise Data Generator\n",
    "data_gen_train = DataGenerator(dataset, features, folds=folds_train,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True, train=True, scaler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also fit a scaler to transform training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler(normalizer=normalizer)\n",
    "print('Fitting features ...')\n",
    "scaler.fit(data_gen_train)\n",
    "print('Done!')\n",
    "\n",
    "data_gen_train.set_scaler(scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise validation data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_val = DataGenerator(dataset, features, folds=folds_val,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False, train=False, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DefineModel\"></a>\n",
    "## 4. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autopool import AutoPool1D\n",
    "from keras.layers import Input, TimeDistributed, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "class DCASE2020Task1Baseline(KerasModelContainer):\n",
    "    \n",
    "    def __init__(self, model=None, model_path=None, metrics=['accuracy'],\n",
    "                 n_frames_cnn=96, n_freq_cnn=64, n_classes=10,\n",
    "                 hidden_layers_size=[512, 128]):\n",
    "        \n",
    "        self.n_frames_cnn = n_frames_cnn \n",
    "        self.n_freq_cnn = n_freq_cnn\n",
    "        self.n_classes = n_classes\n",
    "        self.hidden_layers_size = hidden_layers_size\n",
    "        \n",
    "        super().__init__(model=model, model_path=model_path,\n",
    "                         model_name='DCASE2020Task1Baseline', metrics=metrics)\n",
    "        \n",
    "    def build(self):\n",
    "        # Input\n",
    "        inputs = Input(shape=(self.n_frames_cnn, self.n_freq_cnn), dtype='float32', name='input')\n",
    "\n",
    "        num_hidden_layers = len(self.hidden_layers_size)\n",
    "        # Hidden layers\n",
    "        for idx in range(num_hidden_layers):\n",
    "            if idx == 0:\n",
    "                y = inputs\n",
    "            y = TimeDistributed(Dense(self.hidden_layers_size[idx], activation='relu',\n",
    "                                name='dense_{}'.format(idx+1)))(y)\n",
    "\n",
    "        # Output layer\n",
    "        y = TimeDistributed(Dense(self.n_classes, activation='softmax',\n",
    "                            name='output_t'))(y)\n",
    "\n",
    "        # Apply autopool over time dimension\n",
    "        y = AutoPool1D(axis=1, name='output')(y)\n",
    "\n",
    "        # Create model\n",
    "        self.model = Model(inputs=inputs, outputs=y, name='model')\n",
    "\n",
    "        super().build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"InitModel\"></a>\n",
    "## 5. Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_shape = features.get_shape()\n",
    "n_frames_cnn = features_shape[1]\n",
    "n_freq_cnn = features_shape[2]\n",
    "n_classes = len(dataset.label_list)\n",
    "\n",
    "print(n_frames_cnn, n_freq_cnn, n_classes)\n",
    "model_container = DCASE2020Task1Baseline(model=None, model_path=None, n_classes=n_classes, \n",
    "                                         n_frames_cnn=n_frames_cnn, n_freq_cnn=n_freq_cnn)\n",
    "\n",
    "model_container.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths and save model json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DCASE2020Task1Baseline'\n",
    "mkdir_if_not_exists(model_name)\n",
    "exp_folder = os.path.join(model_name, dataset_name)\n",
    "mkdir_if_not_exists(exp_folder)\n",
    "\n",
    "# save model as json\n",
    "print('saving model to %s' % exp_folder)\n",
    "model_container.save_model_json(exp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arguments = {'early_stopping': early_stopping,\n",
    "                  'epochs': epochs,\n",
    "                  'considered_improvement': considered_improvement,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'batch_size': batch_size,\n",
    "                  'verbose': verbose,\n",
    "                  'optimizer': optimizer}\n",
    "\n",
    "model_container.train(data_gen_train, data_gen_val, weights_path=exp_folder, **train_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best_weights\n",
    "model_container.load_model_weights(exp_folder)\n",
    "\n",
    "# test model\n",
    "X_test, Y_test = data_generator.get_data_for_testing()\n",
    "X_test = scaler.transform(X_test)\n",
    "results = model_container.evaluate(X_test, Y_test)\n",
    "\n",
    "print(results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best_weights\n",
    "model_container.load_model_weights(exp_folder)\n",
    "data_gen_test = DataGenerator(dataset, features, folds=folds_test,\n",
    "                              batch_size='batch_size',\n",
    "                              shuffle=False, train=False, scaler=scaler)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
