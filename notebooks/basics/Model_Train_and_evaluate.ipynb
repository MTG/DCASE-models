{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    " ____   ____    _    ____  _____                          _      _     \n",
    "|  _ \\ / ___|  / \\  / ___|| ____|     _ __ ___   ___   __| | ___| |___ \n",
    "| | | | |     / _ \\ \\___ \\|  _| _____| '_ ` _ \\ / _ \\ / _` |/ _ \\ / __|\n",
    "| |_| | |___ / ___ \\ ___) | |__|_____| | | | | | (_) | (_| |  __/ \\__ \\\n",
    "|____/ \\____/_/   \\_\\____/|_____|    |_| |_| |_|\\___/ \\__,_|\\___|_|___/\n",
    "                                                                        \n",
    "</pre>\n",
    "\n",
    "# DCASE-models Notebooks\n",
    "Python Notebooks for [DCASE-models](https://github.com/pzinemanas/DCASE-models)\n",
    "\n",
    "---\n",
    "### About\n",
    "\n",
    "This notebook shows how to train a model using [DCASE-models](https://github.com/pzinemanas/DCASE-models).\n",
    "\n",
    "### Overview\n",
    "\n",
    "The dataset used is [Urban SED](http://urbansed.weebly.com/), an adaptation of the Convolutional Neural Network (CNN) proposed by Salamon and Bello [[SB-CNN]](http://ieeexplore.ieee.org/document/7829341/) is trained end evaluated.\n",
    "\n",
    "For details on how to download a dataset, extract features or perform data augmentation refer to the respective notebooks.\n",
    "\n",
    "\n",
    "### Organization\n",
    "\n",
    "The Notebook is organized into the following sections.\n",
    "* [1. Prepare data](#Data)\n",
    "* [2. Initialize model](#InitModel)\n",
    "* [3. Train model](#TrainModel)\n",
    "* [4. Evaluate model](#Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "rootdir_path = '../../'\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "sys.path.append(rootdir_path)\n",
    "\n",
    "from dcase_models.data.datasets import get_available_datasets\n",
    "from dcase_models.data.features import get_available_features\n",
    "from dcase_models.model.models import get_available_models\n",
    "from dcase_models.data.data_generator import DataGenerator\n",
    "from dcase_models.data.data_augmentation import AugmentedDataset\n",
    "from dcase_models.data.scaler import Scaler\n",
    "from dcase_models.util.files import load_json\n",
    "from dcase_models.util.files import mkdir_if_not_exists, save_pickle\n",
    "from dcase_models.util.data import evaluation_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data\"></a>\n",
    "## 1. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the following steps could be  ommited if *feature_extraction* and *download_and_prepare_datasets* have already been run. Nonetheless, they have been included to make the notebook self contained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load parameters from json file\n",
    "parameters_file = os.path.join(rootdir_path, 'parameters.json')\n",
    "params = load_json(parameters_file)\n",
    "params_dataset = params['datasets']['URBAN_SED']\n",
    "params_features = params['features']\n",
    "kwargs = {'sequence_hop_time': params_features['sequence_hop_time']}\n",
    "#print dataset parmeters\n",
    "print(json.dumps(params_features, indent=4, sort_keys=True))\n",
    "# print feature extraction parameters \n",
    "print(json.dumps(params_features, indent=4, sort_keys=True))\n",
    "\n",
    "features =  MelSpectrogram(sequence_time=params_features['sequence_time'], \\\n",
    "                                            sequence_hop_time=params_features['sequence_hop_time'], \n",
    "                                            audio_win=params_features['audio_win'], \n",
    "                                            audio_hop=params_features['audio_hop'],  \n",
    "                                            sr=params_features['sr'], \n",
    "                                            **params_features['MelSpectrogram'])\n",
    "dataset = URBAN_SED(os.path.join(rootdir_path, params_dataset[\"dataset_path\"]))\n",
    "\n",
    "if dataset.check_if_downloaded():\n",
    "    print('URBAN_SED dataset was already downloaded. ')\n",
    "else:\n",
    "    print('downloading URBAN_SED dataset')\n",
    "    dataset.download()\n",
    "# Extract features\n",
    "if features.check_if_extracted(dataset):\n",
    "    print('Features were already extracted for URBAN_SED dataset. ')\n",
    "else:\n",
    "    print('Extracting features ...')\n",
    "    features.extract(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise data generators and fit scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test folds\n",
    "folds_train, folds_val, folds_test = evaluation_setup('test', dataset.fold_list,\\\n",
    "                                             params_dataset['evaluation_mode'],\n",
    "                                             use_validate_set=True)\n",
    "#initialise train Data Generator\n",
    "data_gen_train = DataGenerator(dataset, features, folds=folds_train,\\\n",
    "                                batch_size=params_train['batch_size'],\n",
    "                                shuffle=True, train=True, scaler=None)\n",
    "# fit scaler\n",
    "scaler = Scaler(normalizer=params_model['normalizer'])\n",
    "print('Fitting features ...')\n",
    "scaler.fit(data_gen_train)\n",
    "print('Done!')\n",
    "\n",
    "data_gen_train.set_scaler(scaler)\n",
    "\n",
    "#Initialise validation data Generator\n",
    "\n",
    "data_gen_val = DataGenerator(dataset, features, folds=folds_val,\\\n",
    "                             batch_size=params_train['batch_size'],\n",
    "                             shuffle=False, train=False, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Model\"></a>\n",
    "## 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_model = params['models'][SB_CNN_SED]\n",
    "metrics = ['sed']\n",
    "X, y = data_gen_train.get_data_batch(0)\n",
    " \n",
    "n_frames_cnn = X.shape[1]\n",
    "n_freq_cnn = X.shape[2]\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "metrics = ['sed']\n",
    "\n",
    "model_container = SB_CNN_SED(model=None, model_path=None, n_classes=n_classes, \n",
    "                             n_frames_cnn=n_frames_cnn, n_freq_cnn=n_freq_cnn,\n",
    "                             metrics=metrics, **params_model['model_arguments'])\n",
    "\n",
    "model_container.model.summary()\n",
    "model_container = model_class(model=None, model_path=None, n_classes=n_classes,\\\n",
    "                              n_frames_cnn=n_frames_cnn, n_freq_cnn=n_freq_cnn,\n",
    "                              metrics=metrics,\n",
    "                              **params_model['model_arguments'])\n",
    "\n",
    "model_container.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Train\"></a>\n",
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_container.train(data_gen_train, data_gen_val,\\\n",
    "                      label_list=dataset.label_list,\n",
    "                      weights_path=exp_folder, **params['train'],\n",
    "                      sequence_time_sec=params_features['sequence_hop_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Evaluate\"></a>\n",
    "## 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best_weights\n",
    "model_container.load_model_weights(exp_folder)\n",
    "data_gen_test = DataGenerator(dataset, features, folds=folds_test,\\\n",
    "                              batch_size=params_train['batch_size'],\\\n",
    "                              shuffle=False, train=False, scaler=scaler)\n",
    "\n",
    "kwargs = {'sequence_time_sec': params_features['sequence_hop_time'],\n",
    "          'metric_resolution_sec': 1.0}\n",
    "results = model_container.evaluate(data_gen_test, label_list=dataset.label_list, **kwargs)\n",
    "\n",
    "print(results[metrics[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
